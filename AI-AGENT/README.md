# AI-AGENT (Ollama)

Минимальный русскоязычный LLM-слой для тренажёра продаж: чат + "память" через сводку контекста.

## Быстрый старт

1) Установи Ollama: https://ollama.com

2) Подтяни модель (пример, стабильный универсал):

```powershell
ollama pull qwen2.5:7b-instruct
```

3) Установи зависимости (в твоём `.venv`, если используешь общий venv проекта):

```powershell
pip install -r AI-AGENT/requirements.txt
```

4) Запусти:

```powershell
python AI-AGENT/run.py --config AI-AGENT/config.example.json
```

## Режим сервиса (HTTP)

Для модульной архитектуры STT → AI-AGENT → LLM запускай HTTP-сервис:

```powershell
python AI-AGENT/server.py
```

Эндпоинты:
- `POST /chat` — принимает `{ "text": "...", "session_id": "optional", "reset": false }`
- `GET /health` — статус AI-AGENT и LLM

## Настройка языка ответа

По умолчанию AI-AGENT принудительно отвечает **только по‑русски**.
Можно переопределить через `config.json`:

```json
"ai_agent": {
  "system_prompt": "Отвечай строго по-русски. Не добавляй другие языки или иероглифы."
}
```

## Русские модели “как GigaChat/Yandex”

Важно: **GigaChat** и **YandexGPT** — это обычно облачные/закрытые модели по API. Их нельзя "поставить" локально через Ollama как есть.

Что ближе по русскому стилю локально:

- **Saiga / Vikhr (на базе Mistral/Llama)** — часто дают более "нативный" русский, хорошо подходят под диалоги продаж.
  - Вариант A (если модель есть в библиотеке Ollama): `ollama pull <имя_модели>`
  - Вариант B (если нет): скачать GGUF и сделать `Modelfile` (я могу добавить шаблон).

- **Qwen2.5 7B/14B** — очень сильный универсал, русский хороший, часто лучше по удержанию длинного контекста.

Рекомендуемый старт под 3060 Ti: `qwen2.5:7b-instruct` в кванте `Q4_K_M` (Ollama сам подберёт), затем сравнить с Saiga/Vikhr на твоих сценариях.

## Память / контекст

Агент хранит:
- последние реплики в истории
- отдельную сводку контекста (обновляется каждые N ходов)

Это помогает держать смысл (этап сделки, возражения, договорённости), даже если переписка длинная.
